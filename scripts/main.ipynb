{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from key import get_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the OPENAI key\n",
    "client = OpenAI(\n",
    "    api_key= get_key()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' OLD_XML_DIR = \"../layouts/our.v.1\"\\nNEW_XML_DIR = \"../layouts/our.v.2\"\\nOLD_TESTS_DIR = \"../tests/our.v.1\"\\nNEW_TESTS_DIR = \"../tests/our.v.2\" '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folders for our application versions\n",
    "\"\"\" OLD_XML_DIR = \"../layouts/our.v.1\"\n",
    "NEW_XML_DIR = \"../layouts/our.v.2\"\n",
    "OLD_TESTS_DIR = \"../tests/our.v.1\"\n",
    "NEW_TESTS_DIR = \"../tests/our.v.2\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders for Omni-Notes\n",
    "OLD_XML_DIR = \"../layouts/v6.1.0\"\n",
    "NEW_XML_DIR = \"../layouts/v6.3.0\"\n",
    "OLD_TESTS_DIR = \"../tests/v6.1.0\"\n",
    "NEW_TESTS_DIR = \"../tests/v6.3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to read a content of a file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# reading all files from different directories and store their content in a dictionary where the keys are the filenames and the values are the file contents.\n",
    "old_xml_files = {f: read_file(os.path.join(OLD_XML_DIR, f)) for f in os.listdir(OLD_XML_DIR)}\n",
    "new_xml_files = {f: read_file(os.path.join(NEW_XML_DIR, f)) for f in os.listdir(NEW_XML_DIR)}\n",
    "old_test_files = {f: read_file(os.path.join(OLD_TESTS_DIR, f)) for f in os.listdir(OLD_TESTS_DIR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting XMLs into a single string, for LLM input\n",
    "old_xml_data = \"\\n\\n\".join([f\"File: {name}\\n{content}\" for name, content in old_xml_files.items()])\n",
    "new_xml_data = \"\\n\\n\".join([f\"File: {name}\\n{content}\" for name, content in new_xml_files.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML differences detected for screen7.xml\n",
      "XML differences detected for screen6.xml\n",
      "XML differences detected for screen5.xml\n",
      "XML differences detected for screen1.xml\n",
      "XML differences detected for screen2.xml\n",
      "XML differences detected for screen3.xml\n",
      "XML differences detected for screen12.xml\n",
      "XML differences detected for screen11.xml\n",
      "XML differences detected for screen10.xml\n",
      "XML differences detected for screen14.xml\n",
      "XML differences detected for screen15.xml\n",
      "XML differences detected for screen8.xml\n",
      "XML differences detected for screen9.xml\n"
     ]
    }
   ],
   "source": [
    "# first LLM step, finding the XML differences between different GUI versions \n",
    "xml_differences = {}\n",
    "\n",
    "for xml_name, old_xml_content in old_xml_files.items():\n",
    "    new_xml_content = new_xml_files.get(xml_name, \"\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in analyzing Android UI XML layouts.\n",
    "    Here is the old UI XML layout:\n",
    "    {old_xml_content}\n",
    "\n",
    "    Here is the updated UI XML layout:\n",
    "    {new_xml_content}\n",
    "\n",
    "    Please list all UI changes between these two versions.\n",
    "    Format the response as structured text, including:\n",
    "    - Elements that were **added** (with their IDs and attributes).\n",
    "    - Elements that were **removed**.\n",
    "    - Elements that were **renamed**.\n",
    "    - Any **structural changes** (e.g., button moved inside another layout).\n",
    "    - Any **attribute changes** (e.g., text color changed).\n",
    "    - Any **value changes**, checking the text content, checking the text attribute in the every items (e.g., text content changed, for example if the button text changed or the title changed). For example if an item, in the old version, have the attribute \"text=\"Item 1\" and in the new version for the same item the attribute is \"text=\"Item 2\", then it is a value change, so you have to write that as a difference.\n",
    "\n",
    "    Return the response in JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in Android UI structure analysis.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "    xml_differences[xml_name] = response.choices[0].message.content\n",
    "\n",
    "    print(f\"XML differences detected for {xml_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to clean the LLM-generated code, that starts with ```java and end with ```\n",
    "def clean_java_code(java_code):\n",
    "    \"\"\" Remove the code block markers (```java ... ```) \"\"\"\n",
    "    return re.sub(r'```java\\n|\\n```', '', java_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to clean the LLM-generated code, that starts with ```kotlin and end with ```\n",
    "def clean_kotlin_code(kotlin_code):\n",
    "    \"\"\" Remove the code block markers (```kotlin ... ```) \"\"\"\n",
    "    return re.sub(r'```kotlin\\n|\\n```', '', kotlin_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../tests/v6.3.0/CategoryLifecycleTest.java\n",
      "Saved ../tests/v6.3.0/SearchTagBackArrowTest.java\n",
      "Saved ../tests/v6.3.0/NoteLifecycleTest.java\n",
      "Saved ../tests/v6.3.0/FabCameraNoteTest.java\n",
      "Saved ../tests/v6.3.0/AutoBackupTest.java\n",
      "Saved ../tests/v6.3.0/RecurrenceRuleTest.java\n",
      "Saved ../tests/v6.3.0/SettingsActivityTest.java\n",
      "Saved ../tests/v6.3.0/RemindersLifecycleTest.java\n",
      "Saved ../tests/v6.3.0/BaseEspressoTest.java\n",
      "Saved ../tests/v6.3.0/MrJingleLifecycleTest.java\n",
      "Saved ../tests/v6.3.0/NoteListMenuTest.java\n",
      "Saved ../tests/v6.3.0/DrawerMenusEspressoTest.java\n",
      "Saved ../tests/v6.3.0/FabLifecycleTest.java\n"
     ]
    }
   ],
   "source": [
    "# second LLM step, updating the test cases, starting from the old ones, analyzing all the xml changes\n",
    "for test_name, old_test in old_test_files.items():\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in Android UI test automation using Espresso.\n",
    "\n",
    "    The Android app's UI has changed, and test cases need to be updated accordingly.\n",
    "\n",
    "    Here are all the detected UI changes:\n",
    "    {xml_differences}\n",
    "\n",
    "    Here is an old test case:\n",
    "    {old_test}\n",
    "\n",
    "    Please update the test case to ensure it remains valid with the new UI structure.\n",
    "    Ensure that:\n",
    "    - It still verifies the original functionality.\n",
    "    - It interacts with the correct elements based on the updated XML.\n",
    "    - Any removed elements are handled gracefully.\n",
    "    - Any renamed or moved elements are adjusted accordingly.\n",
    "    - It follows best practices for Android UI test automation.\n",
    "\n",
    "    Return only the updated Java test code without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in Android UI test automation.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "    updated_code = response.choices[0].message.content.strip()\n",
    "    cleaned_java_code = clean_java_code(updated_code)\n",
    "    # cleaned_kotlin_code = clean_kotlin_code(updated_code) # this used in case of our GUI, bacause we did our tests in Kotlin\n",
    "\n",
    "    file_path = f\"{NEW_TESTS_DIR}/{test_name}\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(cleaned_java_code)\n",
    "        # f.write(cleaned_kotlin_code) # this used in case of our GUI, bacause we did our tests in Kotlin\n",
    "    print(f\"Saved {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for the goal test cases\n",
    "GOAL_TESTS_DIR = \"../tests/v6.3.0goal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all files from different directories and store their content in a dictionary where the keys are the filenames and the values are the file contents.\n",
    "new_test_files = {f: read_file(os.path.join(NEW_TESTS_DIR, f)) for f in os.listdir(NEW_TESTS_DIR)}\n",
    "goal_test_files = {f: read_file(os.path.join(GOAL_TESTS_DIR, f)) for f in os.listdir(GOAL_TESTS_DIR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: CategoryLifecycleTest.java\n",
      "Precision: 0.9987\n",
      "Recall: 0.8588\n",
      "------------------------------------\n",
      "Test: SearchTagBackArrowTest.java\n",
      "Precision: 0.8596\n",
      "Recall: 0.4558\n",
      "------------------------------------\n",
      "Test: NoteLifecycleTest.java\n",
      "Precision: 0.9790\n",
      "Recall: 0.6619\n",
      "------------------------------------\n",
      "Test: FabCameraNoteTest.java\n",
      "Precision: 1.0000\n",
      "Recall: 0.6452\n",
      "------------------------------------\n",
      "Test: AutoBackupTest.java\n",
      "Precision: 1.0000\n",
      "Recall: 0.8624\n",
      "------------------------------------\n",
      "Test: RecurrenceRuleTest.java\n",
      "Precision: 0.9956\n",
      "Recall: 0.6338\n",
      "------------------------------------\n",
      "Test: SettingsActivityTest.java\n",
      "Precision: 0.9932\n",
      "Recall: 0.8420\n",
      "------------------------------------\n",
      "Test: RemindersLifecycleTest.java\n",
      "Precision: 1.0000\n",
      "Recall: 0.6254\n",
      "------------------------------------\n",
      "Test: BaseEspressoTest.java\n",
      "Precision: 0.9914\n",
      "Recall: 0.7670\n",
      "------------------------------------\n",
      "Test: MrJingleLifecycleTest.java\n",
      "Precision: 0.9699\n",
      "Recall: 0.7655\n",
      "------------------------------------\n",
      "Test: NoteListMenuTest.java\n",
      "Precision: 0.8543\n",
      "Recall: 0.5244\n",
      "------------------------------------\n",
      "Test: DrawerMenusEspressoTest.java\n",
      "Precision: 0.9124\n",
      "Recall: 0.5208\n",
      "------------------------------------\n",
      "Test: FabLifecycleTest.java\n",
      "Precision: 0.9897\n",
      "Recall: 0.5884\n",
      "------------------------------------\n",
      "----- OVERALL METRICS -----\n",
      "Overall Precision: 0.9824\n",
      "Overall Recall: 0.7339\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# function used to compute precision and recall between AI-generated and human-written test cases\n",
    "def compute_precision_recall(ai_test, human_test):\n",
    "    \n",
    "    # function used to tokenize Java code by splitting words, removing special characters, and ignoring case\n",
    "    def tokenize(code):\n",
    "        return re.findall(r'\\w+', code.lower())  # Extract words (ignoring special chars)\n",
    "    \n",
    "    ai_tokens = tokenize(ai_test)\n",
    "    human_tokens = tokenize(human_test)\n",
    "\n",
    "    ai_counter = Counter(ai_tokens)\n",
    "    human_counter = Counter(human_tokens)\n",
    "\n",
    "    # compute intersection: words that appear in both tests\n",
    "    common_tokens = sum((ai_counter & human_counter).values())\n",
    "\n",
    "    precision = common_tokens / sum(ai_counter.values()) if ai_counter else 0\n",
    "    recall = common_tokens / sum(human_counter.values()) if human_counter else 0\n",
    "\n",
    "    return precision, recall, ai_counter, human_counter\n",
    "\n",
    "# function used to compute overall precision and recall across all test cases\n",
    "def compute_overall_precision_recall(total_ai_counter, total_human_counter, total_common_tokens):\n",
    "    overall_precision = total_common_tokens / sum(total_ai_counter.values()) if total_ai_counter else 0\n",
    "    overall_recall = total_common_tokens / sum(total_human_counter.values()) if total_human_counter else 0\n",
    "    return overall_precision, overall_recall\n",
    "\n",
    "total_ai_counter = Counter()\n",
    "total_human_counter = Counter()\n",
    "total_common_tokens = 0\n",
    "\n",
    "# loop through all test cases and compare AI-generated vs. human-written tests\n",
    "test_count = 0\n",
    "for test_name, ai_test in new_test_files.items():\n",
    "    if test_name not in goal_test_files:\n",
    "        print(f\"Goal test for {test_name} not found, skipping...\")\n",
    "        continue  \n",
    "\n",
    "    goal_test = goal_test_files[test_name]\n",
    "\n",
    "    precision, recall, ai_counter, human_counter = compute_precision_recall(ai_test, goal_test)\n",
    "    \n",
    "    print(f\"Test: {test_name}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    total_ai_counter.update(ai_counter)\n",
    "    total_human_counter.update(human_counter)\n",
    "    total_common_tokens += sum((ai_counter & human_counter).values())\n",
    "\n",
    "    test_count += 1\n",
    "\n",
    "overall_precision, overall_recall = compute_overall_precision_recall(\n",
    "    total_ai_counter, total_human_counter, total_common_tokens\n",
    ")\n",
    "\n",
    "print(\"----- OVERALL METRICS -----\")\n",
    "print(f\"Overall Precision: {overall_precision:.4f}\")\n",
    "print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "print(\"---------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
